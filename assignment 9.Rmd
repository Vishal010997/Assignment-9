---
title: "Assignment 9"
output: html_document
date: "2024-04-18"
---

Loading Data:

```{r}
library(readxl)
social_media <- read_excel("C:/Users/Vishal/Downloads/MVA_CLASS_COMBINE.xlsx")
social_media
str(social_media)
social_media_cleaned <- social_media[,-1]

```



changing column names:


```{R}

#changing column names
change_cols_index <- c(2,4,6,8,10,12,14,16,17,18,19,20,21,22,23,24)
change_cols_name <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time", "Application Type", "Interview_call_received", "Networking", "Learning", "Mood_Productivity", "Morning_tireness", "Morning_tireness", "Weekly_Feelings")
colnames(social_media_cleaned)[change_cols_index] <- change_cols_name



social_media_cleaned


```



Cleaning Data:

Cleaning Null values

```{R}
# Convert "NA", "N/A", "n/a", "na", "N.A", "n.a" to 0
social_media_cleaned[social_media_cleaned == "NA" | social_media_cleaned == "N/A" | social_media_cleaned == "na" | social_media_cleaned == "n/a" | social_media_cleaned == "N.A" | social_media_cleaned == "n.a" | social_media_cleaned == "0" | social_media_cleaned == ""] <- NA
social_media_cleaned

```


Null values converted to 0

```{R}
social_media_cleaned[is.na(social_media_cleaned)] <- '0'
social_media_cleaned
```


Keeping relevant columns only:
All time columns + label to predict ("How did you feel enitre week") + Application type

```{R}
# Define a function to convert time strings to decimal hours
convert_to_decimal_hours <- function(time_string) {
# Check if NA values are present
if (any(is.na(time_string))) {
         return(rep(NA, length(time_string)))  # Return NA for NA values
     }
     
# Define a function to convert HH:MM format to decimal hours
     hhmm_to_decimal <- function(hhmm) {
         parts <- as.numeric(strsplit(hhmm, ":")[[1]])  # Split into hours and minutes
         hours <- parts[1]
         minutes <- ifelse(length(parts) > 1, parts[2], 0)  # Handle missing minutes
         total_hours <- hours + minutes / 60
         return(total_hours)
     }
     
# Convert time strings to decimal hours
decimal_hours <- sapply(time_string, function(x) {
         if (grepl("^\\d+:\\d+$", x)) {
             return(hhmm_to_decimal(x))  # Convert HH:MM format
         } else if (grepl("^\\d+\\.\\d+$", x)) {
             return(as.numeric(x))  # Convert decimal format
         } else if (grepl("^\\d+$", x)) {
             return(as.numeric(x))  # Convert whole numbers
         } else {
             return(NA)  # Return NA for other cases
         }
     })
     
     return(decimal_hours)
}

time_columns <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time") 
# Apply the conversion function to all time columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], convert_to_decimal_hours)
 
# Verify the result
str(social_media_cleaned)

#Dropping the name columns
social_media_cleaned <- social_media_cleaned[, -c(1, 3, 5, 7, 9, 11, 13, 15)] 
social_media_cleaned
```

Data Preporcessing:

Replace mean value with null values for data preprocessing

```{R}
# Loop through each column in time_columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], function(x) {
  # Calculate mean of the column excluding NA values
  mean_value <- mean(x, na.rm = TRUE)
  # Replace NA values with the mean
  x[is.na(x)] <- mean_value
  return(x)
})

# Print the updated data frame
print(social_media_cleaned)
```



```{R}
# Find columns with "_Time"
time_columns <- grep("_Time$", names(social_media_cleaned), value = TRUE)
time_columns

# Define additional columns to keep
additional_columns <- c("Morning_tireness", "Application Type")

# Combine time columns and additional columns to keep
columns_to_keep <- c(time_columns, additional_columns)

# Select columns to keep from the dataframe
social_media_subset <- social_media_cleaned[columns_to_keep]
```
```{R}
social_media_subset
```
```{R}
# Load the caret package
library(caret)

# Specify the column names for one-hot encoding, excluding Morning_tireness
columns <- setdiff(names(social_media_subset), "Morning_tireness")

# Create a formula for one-hot encoding excluding Morning_tireness
formula_str <- paste("Morning_tireness ~ .", collapse = " + ")

# Convert the formula string to a formula object
formula <- as.formula(formula_str)

# Create dummy variables
dummy <- dummyVars(formula, data = social_media_subset)

# Apply one-hot encoding
social_media_subset_encoded <- predict(dummy, newdata = social_media_subset)

# Convert the result to a data frame
social_media_subset_encoded <- as.data.frame(social_media_subset_encoded)

# Convert Morning_tireness back to a categorical variable
social_media_subset_encoded$Morning_tireness <- as.factor(social_media_subset$Morning_tireness)


social_media_subset_encoded
```    
```{r}
colnames(social_media_subset_encoded)
```
```{R}
## Exploratory Analysis
attach(social_media_subset_encoded)
xtabs(~ Morning_tireness + `\`Application Type\`Social media`, data = social_media_subset_encoded)

xtabs(~ Morning_tireness + `\`Application Type\`Learning`, data = social_media_subset_encoded)


xtabs(~ Morning_tireness + `\`Application Type\`No Social Media`, data = social_media_subset_encoded)

xtabs(~ Morning_tireness + Instagram_Time, data=social_media_subset_encoded)
xtabs(~ Morning_tireness + Linkedin_Time, data=social_media_subset_encoded)
xtabs(~ Morning_tireness + Snapchat_Time, data=social_media_subset_encoded)
xtabs(~ Morning_tireness + Whatsapp_Time, data=social_media_subset_encoded)
xtabs(~ Morning_tireness + OTT_Time, data=social_media_subset_encoded)
xtabs(~ Morning_tireness + Reddit_Time, data=social_media_subset_encoded)
xtabs(~ Morning_tireness + Youtube_Time, data=social_media_subset_encoded)
```

1) Model Development (2 points)



To assess the model development, we typically look at several key aspects:

Significance of coefficients: We examine the estimates, standard errors, z-values, and p-values of the coefficients to determine if they are statistically significant. In your output, the intercept and some coefficients like OTT_Time and Snapchat_Time have p-values greater than 0.05, suggesting they may not be statistically significant predictors.


Model fit: We evaluate the model fit using deviance statistics (Null and Residual deviance) and AIC (Akaike Information Criterion). Lower values of AIC indicate better model fit. In your output, the residual deviance is lower than the null deviance, indicating that the model explains some of the variance in the data. However, the AIC is relatively high, suggesting that the model may not fit the data well.


Fisher Scoring iterations: The number of iterations performed during the optimization process. Higher numbers may indicate convergence issues.


Interpretation of coefficients: We interpret the coefficients in relation to the response variable. For example, a positive coefficient for Whatsapp_Time suggests that as the time spent on WhatsApp increases, the odds of morning tiredness also increase.


Based on this information, we can conclude that the model may have some limitations in terms of statistical significance and model fit. Further investigation and possibly model refinement may be necessary.
```{r}

logistic_simple <- glm(Morning_tireness  ~ Whatsapp_Time + Reddit_Time + OTT_Time + Snapchat_Time + `\`Application Type\`No Social Media`, data=social_media_subset_encoded, family="binomial")
summary(logistic_simple)
```
2) Model Acceptance (2 points)

Based on the predicted probabilities of morning tiredness from the logistic regression model, we can make some inferences about the likelihood of experiencing morning tiredness for each observation in the dataset.

Here's how we can interpret the predicted probabilities:

Interpretation of probabilities: The predicted probabilities represent the likelihood (ranging from 0 to 1) of an individual experiencing morning tiredness based on the predictor variables in the model.


Threshold for classification: Typically, we use a threshold probability (e.g., 0.5) to classify observations into binary outcomes (e.g., presence or absence of morning tiredness). For example, if the predicted probability is greater than or equal to 0.5, we might classify the observation as experiencing morning tiredness.


Example inference: Let's take the first observation as an example. The predicted probability of morning tiredness is approximately 0.0395. Since this probability is below the threshold (e.g., 0.5), we might infer that this individual is less likely to experience morning tiredness based on the predictor variables in the model.


Consideration of other factors: It's important to note that the predicted probabilities are based on the predictor variables included in the model. Other factors not included in the model may also influence morning tiredness but are not accounted for in these predictions.

```{R}
predicted.data <- data.frame(probability.of.Morning_tireness=logistic_simple$fitted.values,Morning_tireness=social_media_subset_encoded$Morning_tireness)


options(scipen = 999)
predicted.data
```

3) Residual Analysis (2 points)


Coefficients: The coefficients represent the estimated effect of each predictor variable on the log odds of experiencing morning tiredness.

For example:
Positive coefficients (e.g., Instagram_Time, Linkedin_Time) indicate that an increase in the corresponding predictor variable is associated with higher odds of morning tiredness.


Negative coefficients (e.g., Twitter_Time, Whatsapp_Time) indicate that an increase in the corresponding predictor variable is associated with lower odds of morning tiredness.


Standard Errors and z-values: These are used to assess the significance of the coefficients. Typically, we look for small standard errors and z-values greater than 1.96 (for a significance level of 0.05) to indicate statistical significance. 

However, in this case, all z-values are 0, and the standard errors are extremely large, indicating issues with the model estimation.
Null and Residual Deviance: These deviance statistics are used to assess the goodness of fit of the model. The null deviance represents the deviance of the null model (i.e., a model with no predictors), while the residual deviance represents the deviance of the fitted model. A decrease in deviance indicates a better fit.

In this case, the residual deviance is extremely close to zero, suggesting that the model fits the data perfectly, which is highly unlikely and may indicate overfitting or other issues.


AIC (Akaike Information Criterion): AIC is a measure of the relative quality of a statistical model. Lower AIC values indicate better-fitting models. Here, the AIC is relatively high, which contradicts the extremely low residual deviance and suggests potential problems with the model.


Number of Fisher Scoring iterations: The number of iterations performed during the optimization process. Higher numbers may indicate convergence issues.

```{R}


xtabs(~ probability.of.Morning_tireness +  Whatsapp_Time + OTT_Time + Snapchat_Time + Instagram_Time, data=predicted.data)
logistic <- glm(Morning_tireness ~ ., data=social_media_subset_encoded, family="binomial")
summary(logistic)

```


```{R}

## Now calculate the overall "Pseudo R-squared" and its p-value
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
(ll.null - ll.proposed) / ll.null
```
```{R}
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
predicted.data$Morning_tireness <- data.frame(probability.of.Morning_tireness=logistic$fitted.values,Morning_tireness=social_media_subset_encoded$Morning_tireness)
```

4) Prediction (2 points)


```{r}
predicted.data <- predicted.data[order(predicted.data$probability.of.Morning_tireness, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
```


4) Prediction (2 points)
Prediction Results (pdata):
The pdata vector contains the predicted probabilities of having sleep issues (morning tiredness) for each observation in the dataset. These probabilities range from very low (close to 0) to very high (close to 1).


Actual Values of Morning Tiredness (social_media_subset_encoded$Morning_tireness):
The social_media_subset_encoded$Morning_tireness vector contains the actual values of morning tiredness for each observation, categorized as "No" or "Yes".


Inferences:
By comparing the predicted probabilities (pdata) with the actual values of morning tiredness, we can assess the model's performance in predicting sleep issues.


Observations with high predicted probabilities (close to 1) are likely to be classified as "Yes" (having morning tiredness), while those with low predicted probabilities (close to 0) are likely to be classified as "No" (not having morning tiredness).


We can visualize the relationship between the predicted probabilities and the actual values using a plot. In the provided ggplot code, the geom_point function is used to create a scatter plot where the x-axis represents the index of observations, the y-axis represents the predicted probabilities of having sleep issues, and the color of points represents the actual values of morning tiredness.


Interpretation:
Points with a high predicted probability and actual value "Yes" (morning tiredness) indicate successful predictions.
Points with a high predicted probability but actual value "No" suggest false positives (misclassifications).


Points with a low predicted probability and actual value "Yes" suggest false negatives (misclassifications).


The overall performance of the model can be evaluated based on the distribution and alignment of points in the plot.
```{r}

ggplot(data=predicted.data, aes(x=rank, y=probability.of.Morning_tireness)) +
geom_point(aes(color=social_media_subset_encoded$Morning_tireness), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of having sleep issues")

# From Caret
pdata <- predict(logistic,newdata=social_media_subset_encoded,type="response" )
pdata
social_media_subset_encoded$Morning_tireness
```
```{R}
# Install and load the caret package

library(caret)
library(pROC)

pdataF <- as.factor(ifelse(test=as.numeric(pdata>0.5) == 0, yes="Yes", no="No"))

#From e1071::
confusionMatrix(pdataF, social_media_subset_encoded$Morning_tireness)
# From pROC
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE)
par(pty = "s")
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE)
```
```{R}
## NOTE: By default, roc() uses specificity on the x-axis and the values range
## from 1 to 0. This makes the graph look like what we would expect, but the
## x-axis itself might induce a headache. To use 1-specificity (i.e. the
## False Positive Rate) on the x-axis, set "legacy.axes" to TRUE.
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE)
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage")
```

```{r}
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4)
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4)
```
```{R}
## If we want to find out the optimal threshold we can store the
## data used to make the ROC graph in a variable...
roc.info <- roc(social_media_subset_encoded$Morning_tireness, logistic$fitted.values, legacy.axes=TRUE)
str(roc.info)
## tpp = true positive percentage
## fpp = false positive precentage
roc.df <- data.frame(tpp=roc.info$sensitivities*100, fpp=(1 - roc.info$specificities)*100,thresholds=roc.info$thresholds)
roc.df
head(roc.df) 
```
Inferences:
As the threshold decreases from Inf to 0.000000000012405353, the true positive percentage (tpp) tends to decrease, while the false positive percentage (fpp) tends to increase.


Lowering the threshold allows the model to classify more observations as positive, leading to an increase in true positives (tpp). However, this also increases the likelihood of incorrectly classifying negative cases as positive, resulting in higher false positives (fpp).


The optimal threshold depends on the specific context and the trade-off between true positives and false positives that the model can tolerate.

```{R}
## head() will show us the values for the upper right-hand corner of the ROC graph, when the threshold is so low
## (negative infinity) that every single sample is called "obese".
## Thus TPP = 100% and FPP = 100%
tail(roc.df) 
```
```{R}
## tail() will show us the values for the lower left-hand corner
## of the ROC graph, when the threshold is so high (infinity)
## that every single sample is called "not obese".
## Thus, TPP = 0% and FPP = 0%
## now let's look at the thresholds between TPP 60% and 80%
roc.df[roc.df$tpp > 60 & roc.df$tpp < 80,]
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE)
```


```{r}
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE)

```

```{R}
roc(social_media_subset_encoded$Morning_tireness,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE, partial.auc=c(100, 90), auc.polygon = TRUE, auc.polygon.col = "#377eb822", print.auc.x=45)

```


5) Model Accuracy (2 points)


Inference:
Based on the ROC curve and AUC value, we can infer that the logistic regression model (logistic_simple) has good discriminative ability in predicting morning tiredness.


An AUC value of 86.73% indicates that the model performs well above chance (random guessing), suggesting that it effectively separates individuals with morning tiredness from those without.


The ROC curve's position relative to the diagonal (chance line) further confirms the model's performance compared to random guessing.


```{r}
# Lets do two roc plots to understand which model is better
roc(social_media_subset_encoded$Morning_tireness, logistic_simple$fitted.values, plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, print.auc=TRUE)

# Lets add the other graph
plot.roc(social_media_subset_encoded$Morning_tireness, logistic$fitted.values, percent=TRUE, col="#4daf4a", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=40)
legend("bottomright", legend=c("Simple", "Non Simple"), col=c("#377eb8", "#4daf4a"), lwd=4) 

```